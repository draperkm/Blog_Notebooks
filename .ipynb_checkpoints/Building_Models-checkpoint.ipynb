{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93b65f70",
   "metadata": {},
   "source": [
    "# Building Neural Networks in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65313f0a",
   "metadata": {},
   "source": [
    "We will discuss how to build a machine learning algorithm with PyTorch, mainly focusing on two main classes:\n",
    "\n",
    "- `torch.nn.Module`\n",
    "- `torch.nn.Parameter`\n",
    "\n",
    "These two classes contains the methods to call different types of layers and the parameters from these layers. The `torch.nn.Parameter` is a sub-class of the torch.Tensor class and registers the parameters in the objects of the Module class. The two classes, work together and make possible the building of a model. By observing the example below we can notice that models are built in two main phases:\n",
    "\n",
    "- defining the type and the sequence of the layers (`init` method)\n",
    "- defining how the output of each layer is fed forward across the network (`forward` method)\n",
    "\n",
    "In the following example we will build a model by defining a class calles `Example_Model`, and we will implement 2 fully connected layers, the first followed by a ReLU activation function, and the second one from a Softmax function by using the `init` method. The `forward` method will also be defined to establish how the output of each layer will flow through the network.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845853dd",
   "metadata": {},
   "source": [
    "It is important to notice that when we will print the parameters the ***\"requires_grad=True\"*** will let us know that model/layer is keeping track of the gradient of the parameters, therefore it will be able to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6e1830",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cff5444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Example_Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Example_Model, self).__init__()\n",
    "\n",
    "        self.linear1 = torch.nn.Linear(5, 10) # (5 in features, 10 out features)\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.linear2 = torch.nn.Linear(10, 5) # (10 in features, 5 out features)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "dummy_model = Example_Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5144503",
   "metadata": {},
   "source": [
    "We can print the model just by calling the instance `dummy_model` that we created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48e7e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example_Model(\n",
      "  (linear1): Linear(in_features=5, out_features=10, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (softmax): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80471fb",
   "metadata": {},
   "source": [
    "We can print the details of a specific layer by calling the specific layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f111ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=10, out_features=5, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(dummy_model.linear2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa719708",
   "metadata": {},
   "source": [
    "We can print the parameters of the whole model, and we will have respectely a \n",
    "\n",
    "- $5 \\cdot 10$ weight matrix for the first layer\n",
    "- $10$ bias parameters from the 1st layer\n",
    "- $10 \\cdot 5$ weight matrix for the first layer\n",
    "- $5$ bias parameters from the 1st layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d32759ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─Linear: 1-1                            60\n",
      "├─ReLU: 1-2                              --\n",
      "├─Linear: 1-3                            55\n",
      "├─Softmax: 1-4                           --\n",
      "=================================================================\n",
      "Total params: 115\n",
      "Trainable params: 115\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "S1 = summary(dummy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44272013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3265,  0.0876, -0.2340, -0.1507, -0.2088],\n",
      "        [-0.3483, -0.2962,  0.3250, -0.2340,  0.0179],\n",
      "        [-0.1287, -0.0097,  0.4376,  0.1406,  0.3173],\n",
      "        [ 0.1721,  0.1175,  0.0562, -0.0301,  0.2803],\n",
      "        [-0.0160,  0.1330, -0.2040, -0.2861, -0.1951],\n",
      "        [ 0.3858, -0.4261, -0.0258,  0.0161, -0.3093],\n",
      "        [-0.2284,  0.2183,  0.0424, -0.3168, -0.1888],\n",
      "        [ 0.2205,  0.2653,  0.1024,  0.0023, -0.2518],\n",
      "        [ 0.1119,  0.4154, -0.0807, -0.0566,  0.3123],\n",
      "        [-0.3263,  0.0473, -0.3313,  0.3113, -0.1327]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.3164, -0.1308, -0.0712,  0.1400,  0.0017, -0.1389,  0.3883, -0.2497,\n",
      "         0.3083,  0.3729], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2305,  0.0232, -0.2082,  0.2474, -0.2128, -0.0692, -0.2093,  0.2320,\n",
      "          0.2549, -0.2258],\n",
      "        [-0.0389, -0.0073, -0.2676, -0.0863, -0.1706, -0.0307,  0.0819,  0.1069,\n",
      "          0.1141, -0.1011],\n",
      "        [ 0.1174, -0.1820, -0.3103,  0.0081, -0.1000,  0.2045,  0.1277,  0.2375,\n",
      "          0.1232,  0.0685],\n",
      "        [-0.1838,  0.2427,  0.1553,  0.3095,  0.0345,  0.2310, -0.1493,  0.2479,\n",
      "         -0.1499,  0.0968],\n",
      "        [ 0.2249, -0.2928, -0.2919,  0.0759,  0.0650,  0.3080, -0.2481, -0.2310,\n",
      "         -0.0951,  0.0938]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1397,  0.1226,  0.1450, -0.0347, -0.2238], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in dummy_model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928af705",
   "metadata": {},
   "source": [
    "We can print the parameters from a specific layer of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fc8f8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.2305,  0.0232, -0.2082,  0.2474, -0.2128, -0.0692, -0.2093,  0.2320,\n",
      "          0.2549, -0.2258],\n",
      "        [-0.0389, -0.0073, -0.2676, -0.0863, -0.1706, -0.0307,  0.0819,  0.1069,\n",
      "          0.1141, -0.1011],\n",
      "        [ 0.1174, -0.1820, -0.3103,  0.0081, -0.1000,  0.2045,  0.1277,  0.2375,\n",
      "          0.1232,  0.0685],\n",
      "        [-0.1838,  0.2427,  0.1553,  0.3095,  0.0345,  0.2310, -0.1493,  0.2479,\n",
      "         -0.1499,  0.0968],\n",
      "        [ 0.2249, -0.2928, -0.2919,  0.0759,  0.0650,  0.3080, -0.2481, -0.2310,\n",
      "         -0.0951,  0.0938]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1397,  0.1226,  0.1450, -0.0347, -0.2238], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in dummy_model.linear2.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe0181",
   "metadata": {},
   "source": [
    "# Types of layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937c51da",
   "metadata": {},
   "source": [
    "## Linear layers\n",
    "\n",
    "In this kind of layers, every input influences every output, and this is why it is also referred to as \"fully connected\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b73cf2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3315, -0.7732, -0.9124, -0.2660,  0.0634]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "fc = torch.nn.Linear(10, 5)\n",
    "\n",
    "input_vector = torch.rand(1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba51dcf",
   "metadata": {},
   "source": [
    "We will expect an mxn Weight's Matrix if the layer has m inputs and n outputs. Also we will expect a vector representing the Bias of lenght n. In this case therefore we will expect $10 \\cdot 5 + 5 = 55$ total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8363d34a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "└─Linear: 0-1                            55\n",
      "=================================================================\n",
      "Total params: 55\n",
      "Trainable params: 55\n",
      "Non-trainable params: 0\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "Linear_layer = summary(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "266fbcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and Bias parameters:\n",
      "Parameter containing:\n",
      "tensor([[ 0.2668,  0.1165, -0.2509, -0.1113,  0.0656, -0.1682, -0.2554, -0.0639,\n",
      "         -0.2155, -0.1014],\n",
      "        [-0.0579,  0.1696, -0.0950,  0.2347,  0.1333,  0.1858,  0.1879,  0.2299,\n",
      "          0.0347,  0.0622],\n",
      "        [ 0.2834,  0.2979,  0.1138,  0.2028,  0.0224, -0.2499,  0.2711,  0.2238,\n",
      "          0.0653,  0.0706],\n",
      "        [ 0.0846, -0.0308, -0.2874,  0.1763,  0.1755, -0.0564,  0.2247,  0.0661,\n",
      "         -0.1393,  0.1619],\n",
      "        [-0.0236, -0.1671, -0.1087,  0.1301,  0.1188,  0.0835, -0.2109,  0.3161,\n",
      "          0.0082, -0.0635]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1209, -0.1721, -0.0505, -0.1810,  0.0999], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('Weight and Bias parameters:')\n",
    "for param in fc.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cfbc39b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4420,  0.1887,  0.3654,  0.4241, -0.3492]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output_vector = fc(input_vector)\n",
    "\n",
    "print(output_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717d479",
   "metadata": {},
   "source": [
    "## Convolutional layers\n",
    "\n",
    "They are used often in computer vision problems for they capacity of recognizing clusters of features and compose them in larger groups. Let us consider the case of LeNet5 as a network composed by convolutional layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3169dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.functional as F\n",
    "\n",
    "\n",
    "class LeNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel (black & white), 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = torch.nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = torch.nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = torch.nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = torch.nn.Linear(120, 84)\n",
    "        self.fc3 = torch.nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d9b5ae",
   "metadata": {},
   "source": [
    "## Max pooling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28087346",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 6, 6)\n",
    "print(my_tensor)\n",
    "\n",
    "maxpool_layer = torch.nn.MaxPool2d(3)\n",
    "print(maxpool_layer(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4b55d",
   "metadata": {},
   "source": [
    "## Normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27288878",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 4, 4) * 20 + 5\n",
    "print(my_tensor)\n",
    "\n",
    "print(my_tensor.mean())\n",
    "\n",
    "norm_layer = torch.nn.BatchNorm1d(4)\n",
    "normed_tensor = norm_layer(my_tensor)\n",
    "print(normed_tensor)\n",
    "\n",
    "print(normed_tensor.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6cbb8d",
   "metadata": {},
   "source": [
    "## Dropout layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd248e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tensor = torch.rand(1, 4, 4)\n",
    "\n",
    "dropout = torch.nn.Dropout(p=0.4)\n",
    "print(dropout(my_tensor))\n",
    "print(dropout(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e36ee2",
   "metadata": {},
   "source": [
    "## Other types of layers available in PyTorch\n",
    "\n",
    "Please refer to the following page for a complete list of the available layers present in the TORCH.NN module: https://pytorch.org/docs/stable/nn.html#convolution-layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "venv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
